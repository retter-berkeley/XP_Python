{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3568878b-b080-4e95-b2e9-3138639a61ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gymansium for Xplane\n",
    "#taken from https://www.gymlibrary.dev/content/environment_creation/\n",
    "#create gymansium to run XPlane.\n",
    "#start with XPlane running, use with XPPyhton.  Don't see any way to reload situation\n",
    "#will need to init by ??\n",
    "#reset is give new command or set position?\n",
    "#get obs gets state = [x;u]=[heading, alt, airspeed; pitch, roll, throttle]\n",
    "#step by?\n",
    "#to reduce crashes limit commands, faster failure\n",
    "#start with current state (heading/alt/airspeed + control settings pitch, roll, throttle)\n",
    "#try to get to commanded state and stay there, reward based on how far from commanded state normalized\n",
    "#probably make commands a small set of discrete (+/-10 deg hdg change and/or +/-500 alt, airspeed constant)\n",
    "#try to reach from \"random\" current state\n",
    "#need way to time steps--maybe allow input every T=6s sim time, give it 1 sim time minute (at 10x or so sim speed)\n",
    "#make sure situation display settings are turned way down\n",
    "#score reward every 6s, cumulative\n",
    "\n",
    "from os import path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pde\n",
    "import random\n",
    "from XPPython3 import xp\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.classic_control import utils\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "##At the top of the code\n",
    "import logging\n",
    "logger = logging.getLogger('requests_throttler')\n",
    "logger.addHandler(logging.NullHandler())\n",
    "logger.propagate = False\n",
    "\n",
    "\n",
    "#not sure where to put these\n",
    "\n",
    "#diffusion equation is included in py-pde, so don't need seperate dynamics\n",
    "#import dynamics\n",
    "#from dynamics import MGM\n",
    "\n",
    "class XPlaneEnv(gym.Env):\n",
    "    #observation space is state and control spaces, normalized\n",
    "    #space is 7-vector\n",
    "    #also make variables to support XP python calls\n",
    "    #also add menu selection slot for XP python\n",
    "    def __init__(self, render_mode=None, size: int =7):\n",
    "        #gymansium init:  make spaces\n",
    "        obs_size=6\n",
    "        self.observation_space =spaces.Box(low=-1, high=1, shape=(obs_size+2,), dtype=float) #add two for commands\n",
    "        action_size=3\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(action_size,), dtype=float) \n",
    "        #need to update action to normal distribution\n",
    "\n",
    "        self.grid=[]\n",
    "        self.stepper=[]\n",
    "        #XP init:  make variable list\n",
    "        self.StateDataRefDescriptions = [\"sim/flightmodel/engine/ENGN_thro\", \"sim/joystick/yolk_pitch_ratio\",\n",
    "                                   \"sim/joystick/yoke_roll_ratio\",\"sim/joystick/FC_ptch\",\n",
    "                                   \"sim/cockpit2/gauges/indicators/altitude_ft_pilot\", \"sim/flightmodel/position/true_psi\",\n",
    "                                   \"sim/cockpit2/gauges/indicators/airspeed_kts_pilot\"]\n",
    "        self.ActionDataRefDescriptions = [\"sim/flightmodel/engine/ENGN_thro\", \"sim/cockpit2/controls/total_pitch_ratio\",\n",
    "                                   \"sim/cockpit2/controls/total_roll_ratio\",\"sim/flightmodel/controls/rudd_def\"]\n",
    "\n",
    "\n",
    "        #XP call\n",
    "        XPinit(self)\n",
    "        \n",
    "        \n",
    "    def _get_obs(self):\n",
    "\n",
    "        Pitch, Roll, Throttle, AltNorm, HdgNorm, SpeedNorm=XPobs(self)\n",
    "        self.state=np.array([Pitch, Roll, Throttle, AltNorm, HdgNorm, SpeedNorm, self.command[0], self.command[1]])\n",
    "        return self.state\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options=None):\n",
    "        #reset to altitude 4000, airspeed 120, heading 180\n",
    "        #only doing relative heading/altitude changes so shouldn't affect things much\n",
    "        AltRef=xp.findDataRef(\"sim/cockpit2/gauges/indicators/altitude_ft_pilot\")\n",
    "        SpeedRef=xp.findDataRef(\"sim/cockpit2/gauges/indicators/airspeed_kts_pilot\")\n",
    "        HdgRef=xp.findDataRef(\"sim/flightmodel/position/true_psi\")\n",
    "        \n",
    "        xp.setDataf(AltRef, 4000.)\n",
    "        xp.setDataf(SpeedRef, 120.)\n",
    "        xp.setDataf(HdgRef, 180.)\n",
    "\n",
    "        #commands randmoly chosen from a matrix of possible commands \n",
    "        #+/- 45 deg heading, with fixed initial heading this is 225 or 135\n",
    "        #+/- 500 ft elevation, with fixed initial altitude this is 4500 or 3500\n",
    "        #combination of the two\n",
    "        #commands is 2x8 array with heading change, elevation change as each row\n",
    "        #commands = np.array([[225., 4000.], [135., 4000.], [180., 4500.], [180., 3500.], [225., 4500.], [135., 4500.], [225., 3500.], [135., 3500.]])\n",
    "        #issue normalized commands\n",
    "        commands = np.array([[45./180., 0.], [-45./180., 0.], [0., 500./3000], [0., -500./3000], [45./180, 500./3000], \n",
    "                             [-45./180, 500./3000], [45./180, -500./3000], [-45./180, -500./3000]])\n",
    "        #choose a random command to send\n",
    "        n=random.randint(0,7)\n",
    "        self.command = commands[n,]\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        \n",
    "        return observation, self.state, self.command\n",
    "        \n",
    "    def step(self,action):\n",
    "        #design of the training will need to be period of time (30s-1min sim time) where aagent tries to acheive command, then reset position and command\n",
    "        #during each period T agent can continuously control pitch, roll, throttle (leave rudder/yaw out for now and just do uncoordinated)\n",
    "        #if altitude change by more than 1500 feet or heading change by more than 90 degrees, or speed change by more than 50kts then fail\n",
    "        #want to sample for reward and updating control input every X sim seconds (start with T/10)\n",
    "        #what/how does step work then, is it every 6 seconds or an entire period?  Each step should be a period or a full contorl/reward interval\n",
    "        #action to be a numpy 3-vector pitch commmand, roll command, throttle command\n",
    "          \n",
    "        #self.state.data[0,:]=action[0]\n",
    "        state=self.state   \n",
    "        XPaction(action)\n",
    "        observation=self._get_obs()\n",
    "        reward=np.sqrt(10.*(self.state.data[3]-self.command[1])**2+10.*(self.state.data[4]-self.command[0])**2+10.*(self.state.data[5]-0.)**2)\n",
    "        done=False\n",
    "        truncated=False\n",
    "        if (self.state.data[3]>1 or self.state.data[3]<-1 or self.state.data[4]>1 or self.state.data[4]<-1 or \n",
    "            self.state.data[5]>1 or self.state.data[5]<-1 ):\n",
    "            truncated= True\n",
    "            reward=-5000  #if beyond these bounds then consider as departed controlled flight\n",
    "        state=self.state\n",
    " \n",
    "        return self.state, reward, done, truncated, {}\n",
    "    \n",
    "    \n",
    "    def close(self):\n",
    "        XPluginStop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a13a7c-19ed-4c2a-89fa-99cedb7e6197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XP functions to be in python plugin folder\n",
    "from XPPython3 import xp\n",
    "class PythonInterface:\n",
    "    def XPinit(gym):\n",
    "        gym.StateDataRef = []\n",
    "        for Item in range(gym.obs_size):\n",
    "            gym.InputDataRef.append(xp.findDataRef(gym.StateDataRefDescriptions[Item]))\n",
    "    \n",
    "        gym.ActionDataRef = []\n",
    "        for Item in range(gym.action_size):\n",
    "            gym.ActionDataRef.append(xp.findDataRef(gym.ActionDataRefDescriptions[Item]))\n",
    "      \n",
    "        #XP init:  make item list\n",
    "        Item = xp.appendMenuItem(xp.findPluginsMenu(), \"Python - Position 1\", 0)\n",
    "        gym.PositionMenuHandlerCB = self.PositionMenuHandler\n",
    "        gym.Id = xp.createMenu(\"Position1\", xp.findPluginsMenu(), Item, self.PositionMenuHandlerCB, 0)\n",
    "        xp.appendMenuItem(self.Id, \"Position1\", 1)\n",
    "    \n",
    "        # Flag to tell us if the widget is being displayed.\n",
    "        gym.MenuItem1 = 0\n",
    "        return gym\n",
    "    \n",
    "    def XPobs(self):\n",
    "        Pitch=[]\n",
    "        Roll = []\n",
    "        #Yaw = []\n",
    "        Throttle =[]\n",
    "        Alt = []\n",
    "        Hdg = []\n",
    "        Speed = []\n",
    "        count = xp.getDatavf(self.InputDataRef[0], Throttle, 0, 1)\n",
    "    \n",
    "        # #Get pitch, roll, yaw\n",
    "    \n",
    "        Pitch = xp.getDataf(self.InputDataRef[1])#, self.Pitch, 0, 1) #normalized\n",
    "        Roll = xp.getDataf(self.InputDataRef[2])#, self.Roll, 0, 1)\n",
    "        #Yaw = xp.getDataf(self.InputDataRef[3])#, self.Yaw, 0, 1)\n",
    "    \n",
    "        # #Get altitude, heading, airspeed\n",
    "    \n",
    "        Alt = xp.getDataf(self.InputDataRef[4])#, self.Alt, 0, 1)\n",
    "        Hdg = xp.getDataf(self.InputDataRef[5])#, self.Hdg, 0, 1)\n",
    "        Speed = xp.getDataf(self.InputDataRef[6])#, self.Speed, 0, 1)\n",
    "        #convert to degrees and normalize to +/- 1 with guessed limits \n",
    "        SpeedNorm= (Speed-120.)/(170-70) #if speed outside these limits, then fail\n",
    "        AltNorm= (Alt-4000)/(5500-2500) #if alt outside limits, fail\n",
    "        HdgNorm = (Hdg-180)/(270-90) # hdg limis 90-270\n",
    "        #all other already normalized\n",
    "        return(Pitch, Roll, Throttle, SpeedNorm, AltNorm, HdgNorm)\n",
    "    \n",
    "    def XPreset():\n",
    "        xp.registerFlightLoopCallback(self.InputOutputLoopCB, 3.0, 0)\n",
    "        AltRef=xp.findDataRef(\"sim/cockpit2/gauges/indicators/altitude_ft_pilot\")\n",
    "        SpeedRef=xp.findDataRef(\"sim/cockpit2/gauges/indicators/airspeed_kts_pilot\")\n",
    "        HdgRef=xp.findDataRef(\"sim/flightmodel/position/true_psi\")\n",
    "        \n",
    "        xp.setDataf(AltRef, 4000.)\n",
    "        xp.setDataf(SpeedRef, 120.)\n",
    "        xp.setDataf(HdgRef, 180.)\n",
    "    \n",
    "            #send command\n",
    "        ThrottleCmd=xp.findDataRef(\"sim/flightmodel/engine/ENGN_thro\")\n",
    "        PitchCmd=xp.findDataRef(\"sim/cockpit2/controls/total_pitch_ratio\")\n",
    "        RollCmd=xp.findDataRef( \"sim/cockpit2/controls/total_roll_ratio\")\n",
    "        \n",
    "        xp.setDataf(ThrottleCmd, action[0])\n",
    "        xp.setDataf(PitchCmd, action[1])\n",
    "        xp.setDataf(RollCmd, action[2])\n",
    "       \n",
    "\n",
    "    def XPaction(action):\n",
    "        #send command\n",
    "        ThrottleCmd=xp.findDataRef(\"sim/flightmodel/engine/ENGN_thro\")\n",
    "        PitchCmd=xp.findDataRef(\"sim/cockpit2/controls/total_pitch_ratio\")\n",
    "        RollCmd=xp.findDataRef( \"sim/cockpit2/controls/total_roll_ratio\")\n",
    "        \n",
    "        xp.setDataf(ThrottleCmd, action[0])\n",
    "        xp.setDataf(PitchCmd, action[1])\n",
    "        xp.setDataf(RollCmd, action[2])\n",
    "\n",
    "        \n",
    "        #wait X seconds here, then get observation and reward\n",
    "        xp.registerFlightLoopCallback(MyCallback, 3.0, 0)\n",
    "        xp.unregisterFlightLoopCallback(MyCallback, 0)\n",
    "\n",
    "    def MyCallback(lastCall, elapsedTime, counter, refCon):\n",
    "    \n",
    "       #xp.log(f\"{elapsedTime}, {counter}\")\n",
    "    \n",
    "       return 1.0\n",
    "    \n",
    "    def PositionMenuHandler(self, inMenuRef, inItemRef):\n",
    "        # If menu selected create our widget dialog\n",
    "        if inItemRef == 1:\n",
    "            if self.MenuItem1 == 0:\n",
    "                self.CreatePosition(300, 600, 300, 550)\n",
    "                self.MenuItem1 = 1\n",
    "            else:\n",
    "                if not xp.isWidgetVisible(self.PositionWidget):\n",
    "                    xp.showWidget(self.PositionWidget)\n",
    "    def CreatePosition(self, x, y, w, h):\n",
    "        FloatValue = []\n",
    "        for Item in range(self.MAX_ITEMS):\n",
    "            FloatValue.append(xp.getDataf(self.PositionDataRef[Item]))\n",
    "    \n",
    "        # X, Y, Z, Lat, Lon, Alt\n",
    "        DoubleValue = [0.0, 0.0, 0.0]\n",
    "        DoubleValue[0], DoubleValue[1], DoubleValue[2] = xp.localToWorld(FloatValue[0], FloatValue[1], FloatValue[2])\n",
    "        DoubleValue[2] *= 3.28\n",
    "    \n",
    "        x2 = x + w\n",
    "        y2 = y - h\n",
    "        PositionText = []\n",
    "    \n",
    "        # Create the Main Widget window\n",
    "        self.PositionWidget = xp.createWidget(x, y, x2, y2, 1, \"Python - Position Example 1 by Sandy Barbour\", 1,\n",
    "                                              0, xp.WidgetClass_MainWindow)\n",
    "    \n",
    "        # Add Close Box decorations to the Main Widget\n",
    "        xp.setWidgetProperty(self.PositionWidget, xp.Property_MainWindowHasCloseBoxes, 1)\n",
    "\n",
    "    def XPluginStop(self):\n",
    "        if self.MenuItem1 == 1:\n",
    "            xp.destroyWidget(self.PositionWidget, 1)\n",
    "            self.MenuItem1 = 0\n",
    "\n",
    "        xp.destroyMenu(self.Id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3763e9d-b928-4d14-b2c8-147582e84555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gym test\n",
    "env = gym.make('XPlaneROM-v0')\n",
    "obs, state, command = env.reset(seed=random.randint(0,10000))\n",
    "print(command, type(command))\n",
    "action=np.array([0., 0.,])\n",
    "trunc = False\n",
    "while not trunc:\n",
    "    s_next, r, done, trunc, info = env.step(action)\n",
    "    action=action+np.array([0.05,0.05])\n",
    "    print(s_next)\n",
    "print(s_next, r, action, s_next.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e685085-e273-4547-8fef-2f0e48d69345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPO in pytorch as https://www.datacamp.com/tutorial/proximal-policy-optimization\n",
    "#this is for cart pole which has discrete action and 1d action space\n",
    "#may need to try https://github.com/XinJingHao/PPO-Continuous-Pytorch/blob/main/main.py or other for continuous\n",
    "#or something else for multiple action space dimension\n",
    "# will also need to make sure variable formats match gymnaisum\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BackboneNetwork(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dimensions, out_features, dropout):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features, hidden_dimensions)\n",
    "        self.layer2 = nn.Linear(hidden_dimensions, hidden_dimensions)\n",
    "        self.layer3 = nn.Linear(hidden_dimensions, out_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = f.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = f.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, actor, critic):\n",
    "        super().__init__()\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "    def forward(self, state):\n",
    "        action_pred = self.actor(state)\n",
    "        value_pred = self.critic(state)\n",
    "        return action_pred, value_pred\n",
    "\n",
    "def create_agent(hidden_dimensions, dropout):\n",
    "    INPUT_FEATURES = env_train.observation_space.shape[0]\n",
    "    HIDDEN_DIMENSIONS = hidden_dimensions\n",
    "    ACTOR_OUTPUT_FEATURES = env_train.action_space.n\n",
    "    CRITIC_OUTPUT_FEATURES = 1\n",
    "    DROPOUT = dropout\n",
    "    actor = BackboneNetwork(\n",
    "            INPUT_FEATURES, HIDDEN_DIMENSIONS, ACTOR_OUTPUT_FEATURES, DROPOUT)\n",
    "    critic = BackboneNetwork(\n",
    "            INPUT_FEATURES, HIDDEN_DIMENSIONS, CRITIC_OUTPUT_FEATURES, DROPOUT)\n",
    "    agent = ActorCritic(actor, critic)\n",
    "    return agent\n",
    "\n",
    "def calculate_returns(rewards, discount_factor):\n",
    "    returns = []\n",
    "    cumulative_reward = 0\n",
    "    for r in reversed(rewards):\n",
    "        cumulative_reward = r + cumulative_reward * discount_factor\n",
    "        returns.insert(0, cumulative_reward)\n",
    "    returns = torch.tensor(returns)\n",
    "    # normalize the return\n",
    "    returns = (returns - returns.mean()) / returns.std()\n",
    "    return returns\n",
    "\n",
    "def calculate_advantages(returns, values):\n",
    "    advantages = returns - values\n",
    "    # Normalize the advantage\n",
    "    advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "    return advantages\n",
    "\n",
    "def calculate_surrogate_loss(\n",
    "        actions_log_probability_old,\n",
    "        actions_log_probability_new,\n",
    "        epsilon,\n",
    "        advantages):\n",
    "    advantages = advantages.detach()\n",
    "    policy_ratio = (\n",
    "            actions_log_probability_new - actions_log_probability_old\n",
    "            ).exp()\n",
    "    surrogate_loss_1 = policy_ratio * advantages\n",
    "    surrogate_loss_2 = torch.clamp(\n",
    "            policy_ratio, min=1.0-epsilon, max=1.0+epsilon\n",
    "            ) * advantages\n",
    "    surrogate_loss = torch.min(surrogate_loss_1, surrogate_loss_2)\n",
    "    return surrogate_loss\n",
    "\n",
    "def calculate_losses(\n",
    "        surrogate_loss, entropy, entropy_coefficient, returns, value_pred):\n",
    "    entropy_bonus = entropy_coefficient * entropy\n",
    "    policy_loss = -(surrogate_loss + entropy_bonus).sum()\n",
    "    value_loss = f.smooth_l1_loss(returns, value_pred).sum()\n",
    "    return policy_loss, value_loss\n",
    "\n",
    "def init_training():\n",
    "    states = []\n",
    "    actions = []\n",
    "    actions_log_probability = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    return states, actions, actions_log_probability, values, rewards, done, episode_reward\n",
    "\n",
    "def forward_pass(env, agent, optimizer, discount_factor):\n",
    "    states, actions, actions_log_probability, values, rewards, done, episode_reward = init_training()\n",
    "    state = env.reset()\n",
    "    agent.train()\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        states.append(state)\n",
    "        action_pred, value_pred = agent(state)\n",
    "        action_prob = f.softmax(action_pred, dim=-1)\n",
    "        dist = distributions.Categorical(action_prob)\n",
    "        action = dist.sample()\n",
    "        log_prob_action = dist.log_prob(action)\n",
    "        state, reward, done, _ = env.step(action.item())\n",
    "        actions.append(action)\n",
    "        actions_log_probability.append(log_prob_action)\n",
    "        values.append(value_pred)\n",
    "        rewards.append(reward)\n",
    "        episode_reward += reward\n",
    "    states = torch.cat(states)\n",
    "    actions = torch.cat(actions)\n",
    "    actions_log_probability = torch.cat(actions_log_probability)\n",
    "    values = torch.cat(values).squeeze(-1)\n",
    "    returns = calculate_returns(rewards, discount_factor)\n",
    "    advantages = calculate_advantages(returns, values)\n",
    "    return episode_reward, states, actions, actions_log_probability, advantages, returns\n",
    "\n",
    "def update_policy(\n",
    "        agent,\n",
    "        states,\n",
    "        actions,\n",
    "        actions_log_probability_old,\n",
    "        advantages,\n",
    "        returns,\n",
    "        optimizer,\n",
    "        ppo_steps,\n",
    "        epsilon,\n",
    "        entropy_coefficient):\n",
    "    BATCH_SIZE = 128\n",
    "    total_policy_loss = 0\n",
    "    total_value_loss = 0\n",
    "    actions_log_probability_old = actions_log_probability_old.detach()\n",
    "    actions = actions.detach()\n",
    "    training_results_dataset = TensorDataset(\n",
    "            states,\n",
    "            actions,\n",
    "            actions_log_probability_old,\n",
    "            advantages,\n",
    "            returns)\n",
    "    batch_dataset = DataLoader(\n",
    "            training_results_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False)\n",
    "    for _ in range(ppo_steps):\n",
    "        for batch_idx, (states, actions, actions_log_probability_old, advantages, returns) in enumerate(batch_dataset):\n",
    "            # get new log prob of actions for all input states\n",
    "            action_pred, value_pred = agent(states)\n",
    "            value_pred = value_pred.squeeze(-1)\n",
    "            action_prob = f.softmax(action_pred, dim=-1)\n",
    "            probability_distribution_new = distributions.Categorical(\n",
    "                    action_prob)\n",
    "            entropy = probability_distribution_new.entropy()\n",
    "            # estimate new log probabilities using old actions\n",
    "            actions_log_probability_new = probability_distribution_new.log_prob(actions)\n",
    "            surrogate_loss = calculate_surrogate_loss(\n",
    "                    actions_log_probability_old,\n",
    "                    actions_log_probability_new,\n",
    "                    epsilon,\n",
    "                    advantages)\n",
    "            policy_loss, value_loss = calculate_losses(\n",
    "                    surrogate_loss,\n",
    "                    entropy,\n",
    "                    entropy_coefficient,\n",
    "                    returns,\n",
    "                    value_pred)\n",
    "            optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            value_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_policy_loss += policy_loss.item()\n",
    "            total_value_loss += value_loss.item()\n",
    "    return total_policy_loss / ppo_steps, total_value_loss / ppo_steps\n",
    "\n",
    "def evaluate(env, agent):\n",
    "    agent.eval()\n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action_pred, _ = agent(state)\n",
    "            action_prob = f.softmax(action_pred, dim=-1)\n",
    "        action = torch.argmax(action_prob, dim=-1)\n",
    "        state, reward, done, _ = env.step(action.item())\n",
    "        episode_reward += reward\n",
    "    return episode_reward\n",
    "\n",
    "def plot_train_rewards(train_rewards, reward_threshold):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(train_rewards, label='Training Reward')\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Training Reward', fontsize=20)\n",
    "    plt.hlines(reward_threshold, 0, len(train_rewards), color='y')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def plot_test_rewards(test_rewards, reward_threshold):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(test_rewards, label='Testing Reward')\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Testing Reward', fontsize=20)\n",
    "    plt.hlines(reward_threshold, 0, len(test_rewards), color='y')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def plot_losses(policy_losses, value_losses):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(value_losses, label='Value Losses')\n",
    "    plt.plot(policy_losses, label='Policy Losses')\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Loss', fontsize=20)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def run_ppo():\n",
    "    MAX_EPISODES = 500\n",
    "    DISCOUNT_FACTOR = 0.99\n",
    "    REWARD_THRESHOLD = 475\n",
    "    PRINT_INTERVAL = 10\n",
    "    PPO_STEPS = 8\n",
    "    N_TRIALS = 100\n",
    "    EPSILON = 0.2\n",
    "    ENTROPY_COEFFICIENT = 0.01\n",
    "    HIDDEN_DIMENSIONS = 64\n",
    "    DROPOUT = 0.2\n",
    "    LEARNING_RATE = 0.001\n",
    "    train_rewards = []\n",
    "    test_rewards = []\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    agent = create_agent(HIDDEN_DIMENSIONS, DROPOUT)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n",
    "    for episode in range(1, MAX_EPISODES+1):\n",
    "        train_reward, states, actions, actions_log_probability, advantages, returns = forward_pass(\n",
    "                env_train,\n",
    "                agent,\n",
    "                optimizer,\n",
    "                DISCOUNT_FACTOR)\n",
    "        policy_loss, value_loss = update_policy(\n",
    "                agent,\n",
    "                states,\n",
    "                actions,\n",
    "                actions_log_probability,\n",
    "                advantages,\n",
    "                returns,\n",
    "                optimizer,\n",
    "                PPO_STEPS,\n",
    "                EPSILON,\n",
    "                ENTROPY_COEFFICIENT)\n",
    "        test_reward = evaluate(env_test, agent)\n",
    "        policy_losses.append(policy_loss)\n",
    "        value_losses.append(value_loss)\n",
    "        train_rewards.append(train_reward)\n",
    "        test_rewards.append(test_reward)\n",
    "        mean_train_rewards = np.mean(train_rewards[-N_TRIALS:])\n",
    "        mean_test_rewards = np.mean(test_rewards[-N_TRIALS:])\n",
    "        mean_abs_policy_loss = np.mean(np.abs(policy_losses[-N_TRIALS:]))\n",
    "        mean_abs_value_loss = np.mean(np.abs(value_losses[-N_TRIALS:]))\n",
    "        if episode % PRINT_INTERVAL == 0:\n",
    "            print(f'Episode: {episode:3} | \\\n",
    "                  Mean Train Rewards: {mean_train_rewards:3.1f} \\\n",
    "                  | Mean Test Rewards: {mean_test_rewards:3.1f} \\\n",
    "                  | Mean Abs Policy Loss: {mean_abs_policy_loss:2.2f} \\\n",
    "                  | Mean Abs Value Loss: {mean_abs_value_loss:2.2f}')\n",
    "        if mean_test_rewards >= REWARD_THRESHOLD:\n",
    "            print(f'Reached reward threshold in {episode} episodes')\n",
    "            break\n",
    "    plot_train_rewards(train_rewards, REWARD_THRESHOLD)\n",
    "    plot_test_rewards(test_rewards, REWARD_THRESHOLD)\n",
    "    plot_losses(policy_losses, value_losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f1d5ce-9d82-42df-9641-bba740f37e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ppo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
