{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d27cf7-1b74-4e58-9ca1-47b9029a880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gymansium for Mooney Dynmaics ROM\n",
    "#taken from https://www.gymlibrary.dev/content/environment_creation/\n",
    "#create gymansium to pre-train an RL agent that will try and fly a Mooney M20J in XPlane\n",
    "#pre train by using SINDY to make ROM\n",
    "#will need same states and inputs as XPlane Gym\n",
    "#state is:  throttle position, pitch ratio, roll ratio, altitude, heading, airspeed\n",
    "#altitude, heading airspeed normalized\n",
    "#SpeedNorm= (Speed-120.)/(170-70) #if speed outside these limits, then fail\n",
    "#AltNorm= (Alt-4000)/(5500-2500) #if alt outside limits, fail\n",
    "#HdgNorm = (Hdg-180)/(270-90) # hdg limis 90-270\n",
    "\n",
    "from os import path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pde\n",
    "import random\n",
    "#from XPPython3 import xp\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.classic_control import utils\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "##At the top of the code\n",
    "import logging\n",
    "logger = logging.getLogger('requests_throttler')\n",
    "logger.addHandler(logging.NullHandler())\n",
    "logger.propagate = False\n",
    "\n",
    "class XPlaneROMEnv(gym.Env):\n",
    "    #observation space is state and control spaces, normalized\n",
    "    #space is 7-vector\n",
    "\n",
    "    def __init__(self, render_mode=None, size: int =7):\n",
    "        #gymansium init:  make spaces\n",
    "        obs_size=7\n",
    "        self.observation_space =spaces.Box(low=-1, high=1, shape=(obs_size,), dtype=float)\n",
    "        action_size=3\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(action_size,), dtype=float) \n",
    "        #need to update action to normal distribution\n",
    "\n",
    "        self.grid=[]\n",
    "        self.stepper=[]\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        #tbd\n",
    "        \n",
    "        #convert to degrees and normalize to +/- 1 with guessed limits \n",
    "        #for the ROM gym, all will be in normalized\n",
    "        # SpeedNorm= (Speed-120.)/(170-70) #if speed outside these limits, then fail\n",
    "        # AltNorm= (Alt-4000)/(5500-2500) #if alt outside limits, fail\n",
    "        # HdgNorm = (Hdg-180)/(270-90) # hdg limis 90-270\n",
    "        #all other already normalized\n",
    "\n",
    "        #self.state.data=[Pitch, Roll, Throttle, AltNorm, HdgNorm, SpeedNorm]\n",
    "        return self.state\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options=None):\n",
    "        #reset to altitude 4000, airspeed 120, heading 180 or, as normalized, 0,0,0\n",
    "        #pitch, roll to 0.  throttle to ??\n",
    "        self.state= np.array([0., 0., 0.5, 0., 0., 0.])\n",
    "\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        #commands randmoly chosen from a matrix of possible commands \n",
    "        #+/- 45 deg heading, with fixed initial heading this is 225 or 135\n",
    "        #+/- 500 ft elevation, with fixed initial altitude this is 4500 or 3500\n",
    "        #combination of the two\n",
    "        #commands is 2x8 array with heading change, elevation change as each row\n",
    "        #commands = np.array([[225., 4000.], [135., 4000.], [180., 4500.], [180., 3500.], [225., 4500.], [135., 4500.], [225., 3500.], [135., 3500.]])\n",
    "        #issue normalized commands\n",
    "        commands = np.array([[45./180., 0.], [-45./180., 0.], [0., 500./3000], [0., -500./3000], [45./180, 500./3000], \n",
    "                             [-45./180, 500./3000], [45./180, -500./3000], [-45./180, -500./3000]])\n",
    "        #choose a random command to send\n",
    "        n=random.randint(0,7)\n",
    "        self.command = commands[n,]\n",
    "        command=commands[n,]\n",
    "\n",
    "        return observation, self.state, command\n",
    "        \n",
    "    def step(self,action):\n",
    "        #Use discrete equation determined by SINDY to advance state one time step\n",
    "        \n",
    "        #self.state.data[0,:]=action[0]\n",
    "        state=self.state\n",
    "        command = self.command\n",
    "        #[equation here]     \n",
    "        self.state[3]=self.state[3]+action[0]\n",
    "        self.state[5]=self.state[4]+action[1]\n",
    "        #self.state[5]=self.state[4]+action[2]\n",
    "        #wait X seconds here, then get observation and reward\n",
    "        \n",
    "        observation=self._get_obs()\n",
    "        reward=np.sqrt((self.state.data[3]-command[1])**2+(self.state.data[4]-command[0])**2+(self.state.data[5]-120.)**2)\n",
    "        done=False\n",
    "        truncated=False\n",
    "        if (self.state.data[3]>1 or self.state.data[3]<-1 or self.state.data[4]>1 or self.state.data[4]<-1 or \n",
    "            self.state.data[5]>1 or self.state.data[5]<-1 ):\n",
    "            truncated= True\n",
    "            reward=-5000\n",
    "        state=self.state\n",
    " \n",
    "        return self.state, reward, done, truncated, {}\n",
    "    \n",
    "    \n",
    "#    def close(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34c3fba0-21b8-408d-89d8-4369504cac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.25  0.  ] <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m trunc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trunc:\n\u001b[1;32m----> 8\u001b[0m     s_next, r, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m      9\u001b[0m     action\u001b[38;5;241m=\u001b[39maction\u001b[38;5;241m+\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.1\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(s_next, r, action)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\XPlaneROM.py:95\u001b[0m, in \u001b[0;36mXPlaneROMEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m#self.state[5]=self.state[4]+action[2]\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m#wait X seconds here, then get observation and reward\u001b[39;00m\n\u001b[0;32m     94\u001b[0m observation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs()\n\u001b[1;32m---> 95\u001b[0m reward\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msqrt((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m-\u001b[39mcommand[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m-\u001b[39mcommand[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m120.\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     96\u001b[0m done\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     97\u001b[0m truncated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#ROM gym test\n",
    "env = gym.make('XPlaneROM-v0')\n",
    "obs, state, command = env.reset(seed=random.randint(0,10000))\n",
    "print(command, type(command))\n",
    "action=np.array([0., 0.,])\n",
    "trunc = False\n",
    "while not trunc:\n",
    "    s_next, r, done, truncated, info = env.step(action)\n",
    "    action=action+np.array([1.0,1.0])\n",
    "print(s_next, r, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf548b5-f117-46a2-b0bb-59a86219cb18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
